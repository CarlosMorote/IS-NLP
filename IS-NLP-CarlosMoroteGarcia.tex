% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Natural Language Processing with Disaster Tweets. Analysis and classification prediction},
  pdfauthor={Carlos Morote García},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Natural Language Processing with Disaster Tweets. Analysis and
classification prediction}
\author{Carlos Morote García}
\date{}

\begin{document}
\maketitle

\hypertarget{preliminaries}{%
\section{0. Preliminaries}\label{preliminaries}}

Check if the libraries that are going to be used along this library are
installed. In case it is detected that one is not installed, it will be
installed automatically.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"./requirements.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

~

Once we have made sure that the libraries have been installed, they will
be imported in order to run the rest of the notebook.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load of all the libraries}

\FunctionTok{library}\NormalTok{(data.table)}
\FunctionTok{library}\NormalTok{(hunspell)}
\FunctionTok{library}\NormalTok{(qdap)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: qdapDictionaries
\end{verbatim}

\begin{verbatim}
## Loading required package: qdapRegex
\end{verbatim}

\begin{verbatim}
## Loading required package: qdapTools
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'qdapTools'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:data.table':
## 
##     shift
\end{verbatim}

\begin{verbatim}
## Loading required package: RColorBrewer
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'qdap'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:base':
## 
##     Filter, proportions
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(utf8)}
\FunctionTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'dplyr'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:qdapTools':
## 
##     id
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:qdapRegex':
## 
##     explain
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:data.table':
## 
##     between, first, last
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:stats':
## 
##     filter, lag
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(quanteda)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Package version: 3.2.0
## Unicode version: 13.0
## ICU version: 68.2
\end{verbatim}

\begin{verbatim}
## Parallel computing: 8 of 8 threads used.
\end{verbatim}

\begin{verbatim}
## See https://quanteda.io for tutorials and examples.
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'quanteda'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:hunspell':
## 
##     dictionary
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(quanteda.textmodels)}
\FunctionTok{library}\NormalTok{(quanteda.textplots)}

\FunctionTok{library}\NormalTok{(wordcloud)}
\FunctionTok{library}\NormalTok{(tm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: NLP
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'NLP'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:quanteda':
## 
##     meta, meta<-
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:qdap':
## 
##     ngrams
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'tm'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:quanteda':
## 
##     stopwords
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:qdap':
## 
##     as.DocumentTermMatrix, as.TermDocumentMatrix
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(caret)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: ggplot2
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'ggplot2'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:NLP':
## 
##     annotate
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:qdapRegex':
## 
##     %+%
\end{verbatim}

\begin{verbatim}
## Loading required package: lattice
\end{verbatim}

~

We will load the variables, lists and functions defined in the
\texttt{healpers.R} file. This is done in an external file to keep the
notebook as readable as possible.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"./helper.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

~

Finally, we will establish a seed to control the generation of numerous
random samples in order to make the experiments reproducible.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{957735}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

~

\hypertarget{import-data}{%
\section{1. Import data}\label{import-data}}

We load the data sets, both the training set (for which we know the
classification) and the test set (for which we do not know the
classification).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.train }\OtherTok{\textless{}{-}} \FunctionTok{fread}\NormalTok{(}\StringTok{"./data/train.csv"}\NormalTok{)}
\NormalTok{df.test }\OtherTok{\textless{}{-}} \FunctionTok{fread}\NormalTok{(}\StringTok{"./data/test.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

~

As this work is intended to explore natural language processing, we will
eliminate those variables that are not related. In this case we
eliminate for both data sets: \texttt{keyword} and \texttt{location}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Train dataset}
\NormalTok{df.train}\SpecialCharTok{$}\NormalTok{id }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{df.train}\SpecialCharTok{$}\NormalTok{keyword }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{df.train}\SpecialCharTok{$}\NormalTok{location }\OtherTok{\textless{}{-}} \ConstantTok{NULL}

\CommentTok{\# Test dataset}
\NormalTok{df.test}\SpecialCharTok{$}\NormalTok{keyword }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{df.test}\SpecialCharTok{$}\NormalTok{location }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\end{Highlighting}
\end{Shaded}

~

Cast the target variable into a factor

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.train}\SpecialCharTok{$}\NormalTok{target }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(df.train}\SpecialCharTok{$}\NormalTok{target)}
\end{Highlighting}
\end{Shaded}

~

Analyzing very superficially the resulting DataFrame we observe that
there are almost a thousand more cases where the tweet does not
correspond to a natural disaster.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(df.train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      text           target  
##  Length:7613        0:4342  
##  Class :character   1:3271  
##  Mode  :character
\end{verbatim}

~

We check if the character encoding is correct (\emph{utf-8}). In this
case we check that it is in this format.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.train}\SpecialCharTok{$}\NormalTok{text[}\SpecialCharTok{!}\FunctionTok{utf8\_valid}\NormalTok{(df.train}\SpecialCharTok{$}\NormalTok{text)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## character(0)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NFC\_df }\OtherTok{\textless{}{-}} \FunctionTok{utf8\_normalize}\NormalTok{(df.train}\SpecialCharTok{$}\NormalTok{text)}
\FunctionTok{sum}\NormalTok{(NFC\_df }\SpecialCharTok{!=}\NormalTok{ df.train}\SpecialCharTok{$}\NormalTok{text) }\CommentTok{\# It is normalized}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NFC\_df }\OtherTok{\textless{}{-}} \FunctionTok{utf8\_normalize}\NormalTok{(df.test}\SpecialCharTok{$}\NormalTok{text)}
\FunctionTok{sum}\NormalTok{(NFC\_df }\SpecialCharTok{!=}\NormalTok{ df.test}\SpecialCharTok{$}\NormalTok{text) }\CommentTok{\# It is normalized}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

~

\hypertarget{data-and-corpus-preprocessing}{%
\section{2. Data and Corpus
preprocessing}\label{data-and-corpus-preprocessing}}

In this section we will generate the corpus for the training and test
datasets. We will also process these corpus to make them ready and clean
to be analyzed by the subsequent models. Additionally, these corpus will
be used to perform a basic analysis of the texts.

Before we start we will use the \texttt{hunspell} library to detect
grammatical errors in the text. We perform this check since these texts
come from Twitter and since they are not formal texts, but texts from
various sources, it is more than likely that there are multiple
grammatical errors. In addition, on Twitter, due to the limited number
of characters that can be used per tweet, we have to cut words or use
contributions that are not grammatically correct.

The results provided by this method are given in a table format where
TRUE means that \textbf{no} error exists, while FALSE means that
\textbf{yes} error exists. Each value corresponds to a word. This
notebook considers as a word any sequence of characters separated by a
space.

We note that 24308 out of 113650 words have some grammatical error. This
means that \(21\%\) of the words have problems.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Detects spelling errors}

\FunctionTok{summary}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(}\FunctionTok{strsplit}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(df.train}\SpecialCharTok{$}\NormalTok{text), }\AttributeTok{split =} \StringTok{" "}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{hunspell\_check}\NormalTok{() )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Mode   FALSE    TRUE 
## logical   24308   89342
\end{verbatim}

~

On the other hand, the test data set presents 10703 errors out of 48876
words. This is another \(21\%\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(}\FunctionTok{strsplit}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(df.test}\SpecialCharTok{$}\NormalTok{text), }\AttributeTok{split =} \StringTok{" "}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{hunspell\_check}\NormalTok{() )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Mode   FALSE    TRUE 
## logical   10703   38173
\end{verbatim}

~

Next we generate the \textbf{Corpus} using the \texttt{tm} library
method.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.train.corpus.original }\OtherTok{\textless{}{-}} \FunctionTok{Corpus}\NormalTok{(}\FunctionTok{VectorSource}\NormalTok{(df.train}\SpecialCharTok{$}\NormalTok{text))}
\NormalTok{df.test.corpus.original }\OtherTok{\textless{}{-}} \FunctionTok{Corpus}\NormalTok{(}\FunctionTok{VectorSource}\NormalTok{(df.test}\SpecialCharTok{$}\NormalTok{text))}
\end{Highlighting}
\end{Shaded}

~

First we transform all generated tokens, words in this case, to
lowercase.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.train.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.train.corpus.original, }\FunctionTok{content\_transformer}\NormalTok{(tolower))}
\NormalTok{df.test.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.test.corpus.original, }\FunctionTok{content\_transformer}\NormalTok{(tolower))}
\end{Highlighting}
\end{Shaded}

~

Twitter has a tag system to allow quick searches by tags, as well as to
allow grouping tweets by the same topic. These are the hashtags, which
are identified with the hash symbol (\#). These tags may contain useful
information, but by their nature they tend to group multiple words
without spaces, causing the algorithm to detect them as a single one. To
extract the maximum knowledge from these tags we have made use of
regular expressions by which it will detect the different words within a
Hashtag, as long as they are differentiated with the first letter of
each word capitalized. Therefore, the hashtag \#SpainOnFire would be
transformed into the three words that compose them: Spain, On, Fire.
Additionally, they will be transformed into lowercase letters to match
the transformation previously made.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.train.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.train.corpus, }\FunctionTok{content\_transformer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(text)\{}\FunctionTok{gsub}\NormalTok{(}\StringTok{"[\#]\{1,\}([A{-}Z][\^{}A{-}Z]*)+"}\NormalTok{, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{1"}\NormalTok{, text)\}))}
\NormalTok{df.test.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.test.corpus, }\FunctionTok{content\_transformer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(text)\{}\FunctionTok{gsub}\NormalTok{(}\StringTok{"[\#]\{1,\}([A{-}Z][\^{}A{-}Z]*)+"}\NormalTok{, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{1"}\NormalTok{, text)\}))}
\end{Highlighting}
\end{Shaded}

~

The mention of users (made with the @ symbol followed by the user name)
is much more difficult to extract the words they may contain. In many
cases, nicknames do not correlate with reality, since the original names
of people are not unique, many times the numbers are manipulated to
create a user name that is unique. Therefore, usernames will be removed
from the Corpus.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.train.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.train.corpus, }\FunctionTok{content\_transformer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(text)\{}\FunctionTok{gsub}\NormalTok{(}\StringTok{"@}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{S+ "}\NormalTok{, }\StringTok{""}\NormalTok{, text)\}))}
\NormalTok{df.test.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.test.corpus, }\FunctionTok{content\_transformer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(text)\{}\FunctionTok{gsub}\NormalTok{(}\StringTok{"@}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{S+ "}\NormalTok{, }\StringTok{""}\NormalTok{, text)\}))}
\end{Highlighting}
\end{Shaded}

~

The urls as a text source do not provide any useful information since
they could be considered a succession of random characters that only
have in common the beginning (\emph{http\ldots{}}). If one wanted to go
deeper into this problem, these links are reverencing an image on the
web, therefore, they could be extracted and analyzed with other
algorithms in order to generate derived variables. As this is not the
object of this work, the latter will not be implemented. Therefore, the
urls will be eliminated.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.train.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.train.corpus, }\FunctionTok{content\_transformer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(text)\{}\FunctionTok{gsub}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{S*http+}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{S*"}\NormalTok{, }\StringTok{""}\NormalTok{, text)\}))}
\NormalTok{df.test.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.test.corpus, }\FunctionTok{content\_transformer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(text)\{}\FunctionTok{gsub}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{S*http+}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{S*"}\NormalTok{, }\StringTok{""}\NormalTok{, text)\}))}
\end{Highlighting}
\end{Shaded}

~

As was the case with urls, emojis also do not provide relevant
information regarding the text. Hence, they will be removed. The
definition of what is an emoji is composed in the \texttt{healper.R}
file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.train.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.train.corpus, }\FunctionTok{content\_transformer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(text)\{}\FunctionTok{mgsub}\NormalTok{(text, }\AttributeTok{pattern =}\NormalTok{ emojis, }\AttributeTok{replacement =} \StringTok{""}\NormalTok{)\}))}
\NormalTok{df.test.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.test.corpus, }\FunctionTok{content\_transformer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(text)\{}\FunctionTok{mgsub}\NormalTok{(text, }\AttributeTok{pattern =}\NormalTok{ emojis, }\AttributeTok{replacement =} \StringTok{""}\NormalTok{)\}))}
\end{Highlighting}
\end{Shaded}

~

Contractions are often treated as a single token when in fact they
represent two or more tokens. They are also often treated as different
tokens but on their contracted version. This means for example that it
will differentiate between the \emph{is} token and the \emph{'s} token
(from \emph{He's} for example). To solve this, a list of equivalences of
a contraction with its extended version has been made. Based on these
references the captured contractions have been discarded.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.train.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.train.corpus, }\FunctionTok{content\_transformer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(text)\{}\FunctionTok{replace\_contraction}\NormalTok{(text, }\AttributeTok{contraction =}\NormalTok{ contra, }\AttributeTok{sent.cap =} \ConstantTok{FALSE}\NormalTok{)\}))}
\NormalTok{df.test.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.test.corpus, }\FunctionTok{content\_transformer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(text)\{}\FunctionTok{replace\_contraction}\NormalTok{(text, }\AttributeTok{contraction =}\NormalTok{ contra, }\AttributeTok{sent.cap =} \ConstantTok{FALSE}\NormalTok{)\}))}
\end{Highlighting}
\end{Shaded}

~

Finally, a series of typical transformations have been carried out, such
as:

\begin{itemize}
\tightlist
\item
  Eliminating the numbers
\item
  Removing characters and words that define the end of a sentence.
\item
  Removing punctuation symbols
\item
  Removing the necessary sequence of blank characters. That is, between
  words there is only a single space.
\item
  Stem the document
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.train.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.train.corpus, }\FunctionTok{content\_transformer}\NormalTok{(removeNumbers))}
\NormalTok{df.train.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.train.corpus, }\FunctionTok{content\_transformer}\NormalTok{(removeWords), }\FunctionTok{stopwords}\NormalTok{())}
\NormalTok{df.train.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.train.corpus, }\FunctionTok{content\_transformer}\NormalTok{(removePunctuation))}
\NormalTok{df.train.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.train.corpus, }\FunctionTok{content\_transformer}\NormalTok{(stripWhitespace))}
\NormalTok{df.train.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.train.corpus, }\FunctionTok{content\_transformer}\NormalTok{(stemDocument))}

\NormalTok{df.test.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.test.corpus, }\FunctionTok{content\_transformer}\NormalTok{(removeNumbers))}
\NormalTok{df.test.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.test.corpus, }\FunctionTok{content\_transformer}\NormalTok{(removeWords), }\FunctionTok{stopwords}\NormalTok{())}
\NormalTok{df.test.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.test.corpus, }\FunctionTok{content\_transformer}\NormalTok{(removePunctuation))}
\NormalTok{df.test.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.test.corpus, }\FunctionTok{content\_transformer}\NormalTok{(stripWhitespace))}
\NormalTok{df.test.corpus }\OtherTok{\textless{}{-}} \FunctionTok{tm\_map}\NormalTok{(df.test.corpus, }\FunctionTok{content\_transformer}\NormalTok{(stemDocument))}
\end{Highlighting}
\end{Shaded}

~

To conclude this section we will contrast the transformations made by
comparing an original record against a modified one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.train.corpus.original[[}\StringTok{\textquotesingle{}32\textquotesingle{}}\NormalTok{]][[}\StringTok{\textquotesingle{}content\textquotesingle{}}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.train.corpus[[}\StringTok{\textquotesingle{}32\textquotesingle{}}\NormalTok{]][[}\StringTok{\textquotesingle{}content\textquotesingle{}}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "wholesal market ablaz"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.test.corpus.original[[}\StringTok{\textquotesingle{}25\textquotesingle{}}\NormalTok{]][[}\StringTok{\textquotesingle{}content\textquotesingle{}}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "SETTING MYSELF ABLAZE http://t.co/6vMe7P5XhC"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.test.corpus[[}\StringTok{\textquotesingle{}25\textquotesingle{}}\NormalTok{]][[}\StringTok{\textquotesingle{}content\textquotesingle{}}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "set ablaz"
\end{verbatim}

~

\hypertarget{term-document-matrix}{%
\section{3. Term Document Matrix}\label{term-document-matrix}}

In this third section we will generate the Term Document Matrix (TDM).
We will also analyze the most frequent tokens while eliminating the less
frequent tokens to remove irrelevant variables that provide (probably)
the least information to our problem.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tdm }\OtherTok{\textless{}{-}} \FunctionTok{TermDocumentMatrix}\NormalTok{(df.train.corpus, }\AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{weighting =}\NormalTok{ weightTfIdf))}
\NormalTok{tdm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## <<TermDocumentMatrix (terms: 11552, documents: 7613)>>
## Non-/sparse entries: 62545/87882831
## Sparsity           : 100%
## Maximal term length: 49
## Weighting          : term frequency - inverse document frequency (normalized) (tf-idf)
\end{verbatim}

~

First we will eliminate those tokens that are less frequent.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tdm }\OtherTok{\textless{}{-}} \FunctionTok{removeSparseTerms}\NormalTok{(tdm, }\FloatTok{0.99}\NormalTok{)}
\NormalTok{tdm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## <<TermDocumentMatrix (terms: 103, documents: 7613)>>
## Non-/sparse entries: 13233/770906
## Sparsity           : 98%
## Maximal term length: 10
## Weighting          : term frequency - inverse document frequency (normalized) (tf-idf)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{inspect}\NormalTok{(tdm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## <<TermDocumentMatrix (terms: 103, documents: 7613)>>
## Non-/sparse entries: 13233/770906
## Sparsity           : 98%
## Maximal term length: 10
## Weighting          : term frequency - inverse document frequency (normalized) (tf-idf)
## Sample             :
##         Docs
## Terms    2598 3418 3732 5185 6134 6523 7186 7471 7473 7579
##   amp       0    0    0    0    0    0    0    0    0    0
##   bomb      0    0    0    0    0    0    0    0    0    0
##   fatal     0    0    0    0    0    0    0    0    0    0
##   fire      0    0    0    0    0    0    0    0    0    0
##   get       0    0    0    0    0    0    0    0    0    0
##   just      0    0    0    0    0    0    0    0    0    0
##   like      0    0    0    0    0    0    0    0    0    0
##   now       0    0    0    0    0    0    0    0    0    0
##   scream    0    0    0    0    0    0    0    0    0    0
##   will      0    0    0    0    0    0    0    0    0    0
\end{verbatim}

~

We will also visualize in a scatter plot the frequency of all variables.
In this way we can study if there is a big difference between the most
frequent variables and the less frequent ones. We can clearly see that
there is such a differentiation. There are a handful of these tokens
that are much more frequent than the rest, while the rest seem to be
repeated with more or less the same frequency.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frecuencias }\OtherTok{\textless{}{-}} \FunctionTok{rowSums}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(tdm))}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{sort}\NormalTok{(frecuencias, }\AttributeTok{decreasing =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{DataExploration_files/figure-latex/unnamed-chunk-27-1.pdf}

~

We list these tokens more frequently. We note that we are dealing with
words related to natural disasters such as fire, flood or, directly,
disaster.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tail}\NormalTok{(}\FunctionTok{sort}\NormalTok{(frecuencias),}\AttributeTok{n=}\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    wreck    crash     time   disast    peopl      new     love      one 
## 113.0232 113.0912 113.4464 117.3973 118.5550 118.9228 122.2927 125.0148 
##      via     burn      now      amp    fatal     bomb     just     will 
## 126.5078 130.3887 134.4261 134.5883 137.7637 153.9875 179.8730 181.7609 
##      get   scream     fire     like 
## 183.4035 186.8431 204.4429 210.3275
\end{verbatim}

~

We also make use of the word cloud graph to visualize the frequency of
these words.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{freq }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{sort}\NormalTok{(}\FunctionTok{rowSums}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(tdm)), }\AttributeTok{decreasing=}\ConstantTok{TRUE}\NormalTok{))}
\FunctionTok{wordcloud}\NormalTok{(}\FunctionTok{rownames}\NormalTok{(freq), freq[,}\DecValTok{1}\NormalTok{], }\AttributeTok{max.words=}\DecValTok{50}\NormalTok{, }\AttributeTok{colors=}\FunctionTok{brewer.pal}\NormalTok{(}\DecValTok{1}\NormalTok{, }\StringTok{"Dark2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{DataExploration_files/figure-latex/unnamed-chunk-29-1.pdf}

~

\hypertarget{modeling-and-evaluation}{%
\section{4. Modeling and evaluation}\label{modeling-and-evaluation}}

In this final section we will create the models that will be in charge
of discerning whether a text corresponds to a natural disaster or not.
For this we will make use of the models provided by the
\texttt{quanteda.textmodels} library:

\begin{itemize}
\tightlist
\item
  SVM Linear
\item
  SVM
\item
  Naive Bayes
\end{itemize}

~

This library requires the information to be in a specific format.
Therefore, before starting with the models, the corpus will be converted
to a document-feature matrix. At the same time that we do this
conversion we will generate two data sets (train and test) based on the
set from which we know the predictions. Then we will be able to evaluate
our models. The division will be 70\% for training and the remaining
30\% for testing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{upper.bound }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{length}\NormalTok{(df.train.corpus)}\SpecialCharTok{*}\FloatTok{0.7}\NormalTok{,}\DecValTok{0}\NormalTok{)}
\NormalTok{dfm.train.train }\OtherTok{\textless{}{-}} \FunctionTok{dfm}\NormalTok{(}\FunctionTok{corpus}\NormalTok{(df.train.corpus)[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{upper.bound])}
\NormalTok{dfm.train.test }\OtherTok{\textless{}{-}} \FunctionTok{dfm}\NormalTok{(}\FunctionTok{corpus}\NormalTok{(df.train.corpus)[upper.bound}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(}\FunctionTok{corpus}\NormalTok{(df.train.corpus))])}
\end{Highlighting}
\end{Shaded}

~

First we will use the \texttt{textmodel\_svm} model. The way to proceed
with the models will always be the same. The model will be trained, then
the predictions will be generated with the saved data to obtain an
evaluation of the model. Finally, we will obtain a series of metrics
that will inform us how well our model generalizes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.svm }\OtherTok{\textless{}{-}} \FunctionTok{textmodel\_svm}\NormalTok{(dfm.train.train, df.train}\SpecialCharTok{$}\NormalTok{target[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{upper.bound])}

\NormalTok{predictions.svm }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model.svm, }\AttributeTok{newdata=}\NormalTok{dfm.train.test)}

\NormalTok{tab\_class }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(df.train}\SpecialCharTok{$}\NormalTok{target[upper.bound}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(}\FunctionTok{corpus}\NormalTok{(df.train.corpus))], predictions.svm)}
\FunctionTok{confusionMatrix}\NormalTok{(tab\_class, }\AttributeTok{mode =} \StringTok{"everything"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##    predictions.svm
##        0    1
##   0 1017  258
##   1  421  589
##                                           
##                Accuracy : 0.7028          
##                  95% CI : (0.6836, 0.7215)
##     No Information Rate : 0.6293          
##     P-Value [Acc > NIR] : 8.537e-14       
##                                           
##                   Kappa : 0.3873          
##                                           
##  Mcnemar's Test P-Value : 5.068e-10       
##                                           
##             Sensitivity : 0.7072          
##             Specificity : 0.6954          
##          Pos Pred Value : 0.7976          
##          Neg Pred Value : 0.5832          
##               Precision : 0.7976          
##                  Recall : 0.7072          
##                      F1 : 0.7497          
##              Prevalence : 0.6293          
##          Detection Rate : 0.4451          
##    Detection Prevalence : 0.5580          
##       Balanced Accuracy : 0.7013          
##                                           
##        'Positive' Class : 0               
## 
\end{verbatim}

~

We continue by analyzing the \texttt{textmodel\_svmlin} model. The
results obtained with this model are worse in all metrics than the
previously trained support vector machine.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.svmlin }\OtherTok{\textless{}{-}} \FunctionTok{textmodel\_svmlin}\NormalTok{(dfm.train.train, df.train}\SpecialCharTok{$}\NormalTok{target[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{upper.bound])}

\NormalTok{predictions.svmlin }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model.svmlin, }\AttributeTok{newdata=}\NormalTok{dfm.train.test, }\AttributeTok{force =}\NormalTok{ T)}

\NormalTok{tab\_class }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(df.train}\SpecialCharTok{$}\NormalTok{target[upper.bound}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(}\FunctionTok{corpus}\NormalTok{(df.train.corpus))], predictions.svmlin)}
\FunctionTok{confusionMatrix}\NormalTok{(tab\_class, }\AttributeTok{mode =} \StringTok{"everything"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##    predictions.svmlin
##       0   1
##   0 754 521
##   1 378 632
##                                           
##                Accuracy : 0.6066          
##                  95% CI : (0.5862, 0.6267)
##     No Information Rate : 0.5046          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.214           
##                                           
##  Mcnemar's Test P-Value : 2.18e-06        
##                                           
##             Sensitivity : 0.6661          
##             Specificity : 0.5481          
##          Pos Pred Value : 0.5914          
##          Neg Pred Value : 0.6257          
##               Precision : 0.5914          
##                  Recall : 0.6661          
##                      F1 : 0.6265          
##              Prevalence : 0.4954          
##          Detection Rate : 0.3300          
##    Detection Prevalence : 0.5580          
##       Balanced Accuracy : 0.6071          
##                                           
##        'Positive' Class : 0               
## 
\end{verbatim}

~

Finally we tried to train a simpler model such as a Naive Bayes model
(\texttt{textmodel\_nb}).

We observe that it is the model with the best results so far. Moreover,
it has the advantage of being a much simpler model than SVMs and it is
also interpretable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.nb }\OtherTok{\textless{}{-}} \FunctionTok{textmodel\_nb}\NormalTok{(dfm.train.train, df.train}\SpecialCharTok{$}\NormalTok{target[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{upper.bound])}

\NormalTok{predictions.nb }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model.nb, }\AttributeTok{newdata=}\NormalTok{dfm.train.test, }\AttributeTok{force =}\NormalTok{ T)}

\NormalTok{tab\_class }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(df.train}\SpecialCharTok{$}\NormalTok{target[upper.bound}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(}\FunctionTok{corpus}\NormalTok{(df.train.corpus))], predictions.nb)}
\FunctionTok{confusionMatrix}\NormalTok{(tab\_class, }\AttributeTok{mode =} \StringTok{"everything"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##    predictions.nb
##       0   1
##   0 947 328
##   1 232 778
##                                           
##                Accuracy : 0.7549          
##                  95% CI : (0.7367, 0.7724)
##     No Information Rate : 0.516           
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.508           
##                                           
##  Mcnemar's Test P-Value : 5.958e-05       
##                                           
##             Sensitivity : 0.8032          
##             Specificity : 0.7034          
##          Pos Pred Value : 0.7427          
##          Neg Pred Value : 0.7703          
##               Precision : 0.7427          
##                  Recall : 0.8032          
##                      F1 : 0.7718          
##              Prevalence : 0.5160          
##          Detection Rate : 0.4144          
##    Detection Prevalence : 0.5580          
##       Balanced Accuracy : 0.7533          
##                                           
##        'Positive' Class : 0               
## 
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Finally we are going to use the whole training data set
(\texttt{df.train}) to train the models again. In this way we will be
able to generate the predictions with the data set of which we do not
know its classification. After generating these predictions we will be
able to upload them to the Kaggle competition from which this
information comes from and thus know its accuracy.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfm.train }\OtherTok{\textless{}{-}} \FunctionTok{dfm}\NormalTok{(}\FunctionTok{corpus}\NormalTok{(df.train.corpus))}
\NormalTok{dfm.test }\OtherTok{\textless{}{-}} \FunctionTok{dfm}\NormalTok{(}\FunctionTok{corpus}\NormalTok{(df.test.corpus))}

\NormalTok{model.svm }\OtherTok{\textless{}{-}} \FunctionTok{textmodel\_svm}\NormalTok{(dfm.train, df.train}\SpecialCharTok{$}\NormalTok{target)}
\NormalTok{model.svmlin }\OtherTok{\textless{}{-}} \FunctionTok{textmodel\_svmlin}\NormalTok{(dfm.train, df.train}\SpecialCharTok{$}\NormalTok{target)}
\NormalTok{model.nb }\OtherTok{\textless{}{-}} \FunctionTok{textmodel\_nb}\NormalTok{(dfm.train, df.train}\SpecialCharTok{$}\NormalTok{target)}

\NormalTok{predictions.svm }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model.svm, }\AttributeTok{newdata=}\NormalTok{dfm.test)}
\NormalTok{predictions.svmlin }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model.svmlin, }\AttributeTok{newdata=}\NormalTok{dfm.test, }\AttributeTok{force =}\NormalTok{ T)}
\NormalTok{predictions.nb }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model.nb, }\AttributeTok{newdata=}\NormalTok{dfm.test, }\AttributeTok{force =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

~

Once we compute all the predicions we generate a \emph{csv} file to
submitt to Kaggle.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.test.svm }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{id =}\NormalTok{ df.test}\SpecialCharTok{$}\NormalTok{id,}
  \AttributeTok{target =}\NormalTok{ predictions.svm}
\NormalTok{)}

\NormalTok{df.test.svmlin }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{id =}\NormalTok{ df.test}\SpecialCharTok{$}\NormalTok{id,}
  \AttributeTok{target =}\NormalTok{ predictions.svmlin}
\NormalTok{)}

\NormalTok{df.test.nb }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{id =}\NormalTok{ df.test}\SpecialCharTok{$}\NormalTok{id,}
  \AttributeTok{target =}\NormalTok{ predictions.nb}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(df.test.svm,}
           \StringTok{"./output/svm.test.csv"}\NormalTok{,}
           \AttributeTok{sep =} \StringTok{","}\NormalTok{,}
           \AttributeTok{col.names =}\NormalTok{ T,}
           \AttributeTok{row.names =}\NormalTok{ F)}

\FunctionTok{write.csv}\NormalTok{(df.test.svmlin,}
           \StringTok{"./output/svmlin.test.csv"}\NormalTok{,}
           \AttributeTok{sep =} \StringTok{","}\NormalTok{,}
           \AttributeTok{col.names =}\NormalTok{ T,}
           \AttributeTok{row.names =}\NormalTok{ F)}

\FunctionTok{write.csv}\NormalTok{(df.test.nb,}
           \StringTok{"./output/nb.test.csv"}\NormalTok{,}
           \AttributeTok{sep =} \StringTok{","}\NormalTok{,}
           \AttributeTok{col.names =}\NormalTok{ T,}
           \AttributeTok{row.names =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

The accuracy of the submited models are:

\begin{itemize}
\tightlist
\item
  \textbf{SVM}: 0.77597
\item
  \textbf{SVM Linear}: 0.62212
\item
  \textbf{Naive Bayes (nb)}: 0.78915
\end{itemize}

\end{document}
