# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
df.corpus.original <- VCorpus(VectorSource(df.train$text))
df.corpus <- tm_map(df.corpus.original, content_transformer(tolower))
# 1
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
# 32
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
# 32
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
# 40
df.corpus <- tm_map(df.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.corpus <- tm_map(df.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.corpus <- tm_map(df.corpus, content_transformer(removeNumbers))
df.corpus <- tm_map(df.corpus, content_transformer(removeWords), stopwords())
df.corpus <- tm_map(df.corpus, content_transformer(removePunctuation))
df.corpus <- tm_map(df.corpus, content_transformer(stripWhitespace))
df.corpus <- tm_map(df.corpus, content_transformer(stemDocument))
df.corpus.original[['32']][['content']]
df.corpus[['32']][['content']]
df.corpus <- tm_map(df.corpus, content_transformer(removeNumbers))
df.corpus <- tm_map(df.corpus, content_transformer(removeWords), stopwords())
df.corpus <- tm_map(df.corpus, content_transformer(removePunctuation))
df.corpus <- tm_map(df.corpus, content_transformer(stripWhitespace))
df.corpus <- tm_map(df.corpus, content_transformer(stemDocument))
df.corpus.original[['32']][['content']]
df.corpus[['32']][['content']]
TermDocumentMatrix(df.corpus, control = list(weighting = weightTfIdf))
tdm <- TermDocumentMatrix(df.corpus, control = list(weighting = weightTfIdf))
tdm
View(tdm)
frecuencias <- rowSums(as.matrix(tdm))
plot(sort(frecuencias, decreasing = TRUE), col='#2F3456', xlab='TF-IDF-based rank', ylab = 'TF-IDF')
plot(sort(frecuencias, decreasing = TRUE))
paste('10 most relevant words in English:', tail(sort(frecuencias),n=10))
paste('Top 10:', tail(sort(frecuencias),n=10))
tail(sort(frecuencias),n=10)
tail(sort(frecuencias, decreasing = T),n=10)
tail(sort(frecuencias),n=10)
dtm <- DocumentTermMatrix(df.corpus)
findFreqTerms(dtm, lowfreq = 20)
findFreqTerms(dtm, lowfreq = 1000)
findFreqTerms(dtm, lowfreq = 100)
findAssocs(dtmEn, terms = findFreqTerms(dtm, lowfreq = 100), corlimit = 0.15)
findAssocs(dtm, terms = findFreqTerms(dtm, lowfreq = 100), corlimit = 0.15)
model.svm <- textmodel_svmlin(dtm, df_train$text)
dfm.train <- dfm(df.corpus)
dfm.train <- dfm(tdm)
dfm.train <- dfm(tokens(df.corpus))
freq <- findFreqTerms(tdm, lowfreq = 75, highfreq = Inf)
freq <- as.matrix(tdm[freq,])
freq <- as.data.frame(freq)
freq <- findFreqTerms(tdm, lowfreq = 75, highfreq = Inf)
freq <- as.matrix(tdm[freq,])
freq <- as.data.frame(freq)
freq <- findFreqTerms(tdm, lowfreq = 75, highfreq = Inf)
freq <- as.matrix(tdm[freq,])
freq <- as.data.frame(freq)
freq <- findFreqTerms(tdm, lowfreq = 75, highfreq = Inf)
freq <- as.matrix(tdm[freq,])
freq <- as.data.frame(freq)
freq <- findFreqTerms(tdm, lowfreq = 75, highfreq = Inf)
freq <- as.matrix(tdm[freq,])
head(freq)
View(freq)
freq <- as.data.frame(rowSums(freq))
colnames(freq) <- "num"
freq$word <- rownames(freq)
head(freq)
View(freq)
options(repr.plot.width = 18, repr.plot.height = 8)
set.seed(5555)
freq$angle <- 45 * sample(-2:2, nrow(freq), replace = TRUE, prob = c(1, 0, 4, 0, 1))
ggplot(freq, aes(label = word,
size = num,
color = num,
angle = angle)) +
geom_text_wordcloud(shape = "circle",
rm_outside = TRUE,
area_corr = F,
rstep = .01,
max_grid_size = 256,
grid_size = 7,
grid_margin = .4
) +
scale_size_area(max_size = 12.5) +
theme_minimal() +
scale_color_gradient(low = "darkgrey", high = "#53d1b1")
library(ggwordcloud)
options(repr.plot.width = 18, repr.plot.height = 8)
set.seed(5555)
freq$angle <- 45 * sample(-2:2, nrow(freq), replace = TRUE, prob = c(1, 0, 4, 0, 1))
ggplot(freq, aes(label = word,
size = num,
color = num,
angle = angle)) +
geom_text_wordcloud(shape = "circle",
rm_outside = TRUE,
area_corr = F,
rstep = .01,
max_grid_size = 256,
grid_size = 7,
grid_margin = .4
) +
scale_size_area(max_size = 12.5) +
theme_minimal() +
scale_color_gradient(low = "darkgrey", high = "#53d1b1")
# Load of all the libraries
set.seed(957735)
library(data.table)
#library(tm)
#library(ggplot2)
#library(ggwordcloud)
#library(keras)
#library(tensorflow)
#library(plotly)
#library(htmlwidgets)
#library(IRdisplay)
library(hunspell)
#library(gridExtra)
#library(DT)
library(qdap)
library(utf8)
library(dplyr)
library(spacyr)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(tm)
source("./helper.R")
df.train <- fread("./data/train.csv")
df.train$id <- NULL
df.train$keyword <- NULL
df.train$location <- NULL
df.train = df.train[df.train$target != ""] # Remove those instances that has is class as NA
df.train$target <- as.factor(df.train$target)
df.test <- fread("./data/test.csv")
df.test$id <- NULL
df.test$keyword <- NULL
df.test$location <- NULL
summary(df.train)
df.train$text[!utf8_valid(df.train$text)]
NFC_df <- utf8_normalize(df.train$text)
sum(NFC_df != df.train$text) # It is normalized
# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
df.corpus.original <- Corpus(VectorSource(df.train$text))
df.corpus <- tm_map(df.corpus.original, content_transformer(tolower))
# 1
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
# 32
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
# 32
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
# 40
df.corpus <- tm_map(df.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.corpus <- tm_map(df.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.corpus <- tm_map(df.corpus, content_transformer(removeNumbers))
df.corpus <- tm_map(df.corpus, content_transformer(removeWords), stopwords())
df.corpus <- tm_map(df.corpus, content_transformer(removePunctuation))
df.corpus <- tm_map(df.corpus, content_transformer(stripWhitespace))
df.corpus <- tm_map(df.corpus, content_transformer(stemDocument))
df.corpus.original[['32']][['content']]
df.corpus[['32']][['content']]
tdm <- TermDocumentMatrix(df.corpus, control = list(weighting = weightTfIdf))
tdm
frecuencias <- rowSums(as.matrix(tdm))
plot(sort(frecuencias, decreasing = TRUE))
tail(sort(frecuencias),n=10)
# Load of all the libraries
set.seed(957735)
library(data.table)
#library(tm)
#library(ggplot2)
#library(ggwordcloud)
#library(keras)
#library(tensorflow)
#library(plotly)
#library(htmlwidgets)
#library(IRdisplay)
library(hunspell)
#library(gridExtra)
#library(DT)
library(qdap)
library(utf8)
library(dplyr)
library(spacyr)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(tm)
source("./helper.R")
df.train <- fread("./data/train.csv")
df.train$id <- NULL
df.train$keyword <- NULL
df.train$location <- NULL
df.train = df.train[df.train$target != ""] # Remove those instances that has is class as NA
df.train$target <- as.factor(df.train$target)
df.test <- fread("./data/test.csv")
df.test$id <- NULL
df.test$keyword <- NULL
df.test$location <- NULL
summary(df.train)
df.train$text[!utf8_valid(df.train$text)]
NFC_df <- utf8_normalize(df.train$text)
sum(NFC_df != df.train$text) # It is normalized
# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
df.corpus.original <- Corpus(VectorSource(df.train$text))
df.corpus <- tm_map(df.corpus.original, content_transformer(tolower))
# 1
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
# 32
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
# 32
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
# 40
df.corpus <- tm_map(df.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.corpus <- tm_map(df.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.corpus <- tm_map(df.corpus, content_transformer(removeNumbers))
df.corpus <- tm_map(df.corpus, content_transformer(removeWords), stopwords())
df.corpus <- tm_map(df.corpus, content_transformer(removePunctuation))
df.corpus <- tm_map(df.corpus, content_transformer(stripWhitespace))
df.corpus <- tm_map(df.corpus, content_transformer(stemDocument))
df.corpus.original[['32']][['content']]
df.corpus[['32']][['content']]
tdm <- TermDocumentMatrix(df.corpus, control = list(weighting = weightTfIdf))
tdm
frecuencias <- rowSums(as.matrix(tdm))
plot(sort(frecuencias, decreasing = TRUE))
tail(sort(frecuencias),n=10)
text <- unlist(df.corpus)
text <- as.data.frame(text)
text <- text[1:(nrow(text)-1), ]
text <- as.data.frame(text)
tail(text)
val <- data.frame(df.train$target, text$text)
colnames(val) <- c("target", "text")
head(val)
ttPart <- .7
u <- runif(n = nrow(val), min = 0, max = 1)
tr_val <- val[u <= ttPart,]
te_val <- val[u > ttPart,]
voc_size <- tr_val$text %>%
as.character() %>%
paste(., collapse = " ") %>%
strsplit(., split = " ") %>%
unlist() %>%
factor() %>%
unique() %>%
length()
voc_size <- tr_val$text %>%
as.character() %>%
paste(., collapse = " ") %>%
strsplit(., split = " ") %>%
unlist() %>%
factor() %>%
unique() %>%
length()
tokenizer <- text_tokenizer(num_words = voc_size)
library(data.table)
library(tm)
library(ggplot2)
library(ggwordcloud)
library(keras)
library(tensorflow)
library(plotly)
library(htmlwidgets)
library(IRdisplay)
library(hunspell)
library(gridExtra)
library(DT)
voc_size <- tr_val$text %>%
as.character() %>%
paste(., collapse = " ") %>%
strsplit(., split = " ") %>%
unlist() %>%
factor() %>%
unique() %>%
length()
tokenizer <- text_tokenizer(num_words = voc_size)
# Load of all the libraries
set.seed(957735)
library(data.table)
#library(tm)
#library(ggplot2)
#library(ggwordcloud)
library(keras)
#library(tensorflow)
#library(plotly)
#library(htmlwidgets)
#library(IRdisplay)
library(hunspell)
#library(gridExtra)
#library(DT)
library(qdap)
library(utf8)
library(dplyr)
library(spacyr)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(tm)
source("./helper.R")
df.train <- fread("./data/train.csv")
df.train$id <- NULL
df.train$keyword <- NULL
df.train$location <- NULL
df.train = df.train[df.train$target != ""] # Remove those instances that has is class as NA
df.train$target <- as.factor(df.train$target)
df.test <- fread("./data/test.csv")
df.test$id <- NULL
df.test$keyword <- NULL
df.test$location <- NULL
summary(df.train)
df.train$text[!utf8_valid(df.train$text)]
NFC_df <- utf8_normalize(df.train$text)
sum(NFC_df != df.train$text) # It is normalized
# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
# Load of all the libraries
set.seed(957735)
library(data.table)
#library(tm)
#library(ggplot2)
#library(ggwordcloud)
library(keras)
#library(tensorflow)
#library(plotly)
#library(htmlwidgets)
#library(IRdisplay)
library(hunspell)
#library(gridExtra)
#library(DT)
library(qdap)
library(utf8)
library(dplyr)
library(spacyr)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(tm)
source("./helper.R")
df.train <- fread("./data/train.csv")
df.train$id <- NULL
df.train$keyword <- NULL
df.train$location <- NULL
df.train = df.train[df.train$target != ""] # Remove those instances that has is class as NA
df.train$target <- as.factor(df.train$target)
df.test <- fread("./data/test.csv")
df.test$id <- NULL
df.test$keyword <- NULL
df.test$location <- NULL
summary(df.train)
df.train$text[!utf8_valid(df.train$text)]
NFC_df <- utf8_normalize(df.train$text)
sum(NFC_df != df.train$text) # It is normalized
# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
df.corpus.original <- Corpus(VectorSource(df.train$text))
df.corpus <- tm_map(df.corpus.original, content_transformer(tolower))
# 1
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
df.corpus <- tm_map(df.corpus.original, content_transformer(tolower))
# 1
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
df.corpus.original <- Corpus(VectorSource(df.train$text))
df.corpus <- tm_map(df.corpus.original, content_transformer(tolower))
View(df.corpus)
# 1
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
# 32
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
# 32
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
# 40
df.corpus <- tm_map(df.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.corpus <- tm_map(df.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.corpus <- tm_map(df.corpus, content_transformer(removeNumbers))
df.corpus <- tm_map(df.corpus, content_transformer(removeWords), stopwords())
df.corpus <- tm_map(df.corpus, content_transformer(removePunctuation))
df.corpus <- tm_map(df.corpus, content_transformer(stripWhitespace))
df.corpus <- tm_map(df.corpus, content_transformer(stemDocument))
df.corpus.original[['32']][['content']]
df.corpus[['32']][['content']]
tdm <- TermDocumentMatrix(df.corpus, control = list(weighting = weightTfIdf))
tdm
frecuencias <- rowSums(as.matrix(tdm))
plot(sort(frecuencias, decreasing = TRUE))
tail(sort(frecuencias),n=10)
text <- unlist(df.corpus)
text <- as.data.frame(text)
text <- text[1:(nrow(text)-1), ]
text <- as.data.frame(text)
tail(text)
val <- data.frame(df.train$target, text$text)
colnames(val) <- c("target", "text")
head(val)
ttPart <- .7
u <- runif(n = nrow(val), min = 0, max = 1)
tr_val <- val[u <= ttPart,]
te_val <- val[u > ttPart,]
voc_size <- tr_val$text %>%
as.character() %>%
paste(., collapse = " ") %>%
strsplit(., split = " ") %>%
unlist() %>%
factor() %>%
unique() %>%
length()
tokenizer <- text_tokenizer(num_words = voc_size)
n
tokenizer %>%  fit_text_tokenizer(tr_val$text)
tokenizer$word_index %>% head(., n=6)
batch_size = 128
eps = 45
latent_size = 3
clip = 3
leRa = .0025
dec = .00001
text_seqs_val_tr <- texts_to_sequences(tokenizer, tr_val$text)
text_seqs_val_te <- texts_to_sequences(tokenizer, te_val$text)
head(text_seqs_val_tr); head(text_seqs_val_te)
inputShape = 120
x_train <- text_seqs_val_tr %>% pad_sequences(maxlen = inputShape, padding = "post")
x_test <- text_seqs_val_te %>% pad_sequences(maxlen = inputShape, padding = "post")
dim(x_train); dim(x_test)
head(x_train); head(x_test)
text_seqs_val_tr <- texts_to_sequences(tokenizer, tr_val$text)
text_seqs_val_te <- texts_to_sequences(tokenizer, te_val$text)
inputShape = 120
x_train <- text_seqs_val_tr %>% pad_sequences(maxlen = inputShape, padding = "post")
x_test <- text_seqs_val_te %>% pad_sequences(maxlen = inputShape, padding = "post")
dim(x_train); dim(x_test)
y_train <- tr_val$target
batch_size = 128
eps = 45
latent_size = 3
clip = 3
leRa = .0025
dec = .00001
ENCODER_in <- layer_input(shape = c(inputShape))
ENCODER_out = ENCODER_in %>%
layer_embedding(input_dim = voc_size,
output_dim = 30) %>%
layer_dropout(.2) %>%
layer_global_max_pooling_1d() %>%
layer_batch_normalization() %>%
layer_dense(units=120) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=60) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=latent_size) %>%
layer_activation_leaky_relu()
ENCODER = keras_model(ENCODER_in, ENCODER_out)
summary(ENCODER)
DECODER_in = layer_input(shape = latent_size)
DECODER_out = DECODER_in %>%
layer_batch_normalization() %>%
layer_dense(units=60) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=120) %>%
layer_activation_leaky_relu() %>%
layer_dense(units = inputShape, activation = "relu") # Because output must be >= 0
DECODER = keras_model(DECODER_in, DECODER_out)
summary(DECODER)
AEN_in = layer_input(shape = inputShape)
AEN_out = AEN_in %>%
ENCODER() %>%
DECODER()
AEN = keras_model(AEN_in, AEN_out)
summary(AEN)
rmse <- function(y_pred, y_true){
y_pred = k_cast(y_pred, dtype="float32")
y_true = k_cast(y_true, dtype="float32")
rmse = k_sqrt(k_mean(k_square(y_pred - y_true), axis=-1))
return(rmse)
}
AEN %>% keras::compile(
loss = rmse,
optimizer = optimizer_adamax(beta_2 = .95, lr = leRa, decay = dec),
metrics = c('accuracy')
)
AEN %>% keras::compile(
loss = rmse,
optimizer = optimizer_adamax(beta_2 = .95, learning_rate = leRa, decay = dec),
metrics = c('accuracy')
)
stop <- callback_early_stopping(monitor = 'accuracy', patience = 10)
hist <- AEN %>%
fit(
x_train,
x_train,
batch_size = batch_size,
epochs = eps,
callbacks = c(stop)
)
encoded = ENCODER %>%
predict(x_train)
decoded = DECODER %>%
predict(encoded) %>%
as.data.frame()
head(decoded)
View(x_train)
