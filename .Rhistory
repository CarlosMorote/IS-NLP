df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removeNumbers))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removeWords), stopwords())
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removePunctuation))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(stripWhitespace))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(stemDocument))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removeNumbers))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removeWords), stopwords())
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removePunctuation))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(stripWhitespace))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(stemDocument))
df.train.corpus.original[['32']][['content']]
df.train.corpus[['32']][['content']]
df.test.corpus.original[['32']][['content']]
df.test.corpus[['32']][['content']]
df.test.corpus.original[['22']][['content']]
df.test.corpus[['22']][['content']]
df.test.corpus.original[['21']][['content']]
df.test.corpus[['21']][['content']]
tolower("@")
df.test.corpus.original[['20']][['content']]
df.test.corpus[['20']][['content']]
df.test.corpus.original[['25']][['content']]
df.test.corpus[['25']][['content']]
tdm <- TermDocumentMatrix(df.train.corpus, control = list(weighting = weightTfIdf))
tdm
frecuencias <- rowSums(as.matrix(tdm))
plot(sort(frecuencias, decreasing = TRUE))
tail(sort(frecuencias),n=20)
inspect(tdm)
tdm <- removeSparseTerms(tdm, 0.99)
tdm
freq = data.frame(sort(colSums(as.matrix(tdm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
library(wordcloud)
freq = data.frame(sort(colSums(as.matrix(tdm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
freq = data.frame(sort(rowSums(as.matrix(tdm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
freq = data.frame(sort(rowSums(as.matrix(tdm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], colors=brewer.pal(1, "Dark2"))
freq = data.frame(sort(rowSums(as.matrix(tdm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
source("./requirements.R")
# Load of all the libraries
library(data.table)
library(hunspell)
library(qdap)
library(utf8)
library(dplyr)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(wordcloud)
library(tm)
library(caret)
source("./helper.R")
set.seed(957735)
df.train <- fread("./data/train.csv")
df.test <- fread("./data/test.csv")
# Train dataset
df.train$id <- NULL
df.train$keyword <- NULL
df.train$location <- NULL
# Test dataset
df.test$keyword <- NULL
df.test$location <- NULL
df.train$target <- as.factor(df.train$target)
summary(df.train)
df.train$text[!utf8_valid(df.train$text)]
NFC_df <- utf8_normalize(df.train$text)
sum(NFC_df != df.train$text) # It is normalized
NFC_df <- utf8_normalize(df.test$text)
sum(NFC_df != df.test$text) # It is normalized
# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
summary(unlist(strsplit(as.character(df.test$text), split = " ")) %>%
hunspell_check() )
df.train.corpus.original <- Corpus(VectorSource(df.train$text))
df.test.corpus.original <- Corpus(VectorSource(df.test$text))
df.train.corpus <- tm_map(df.train.corpus.original, content_transformer(tolower))
df.test.corpus <- tm_map(df.test.corpus.original, content_transformer(tolower))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removeNumbers))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removeWords), stopwords())
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removePunctuation))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(stripWhitespace))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(stemDocument))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removeNumbers))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removeWords), stopwords())
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removePunctuation))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(stripWhitespace))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(stemDocument))
df.train.corpus.original[['32']][['content']]
df.train.corpus[['32']][['content']]
df.test.corpus.original[['25']][['content']]
df.test.corpus[['25']][['content']]
tdm <- TermDocumentMatrix(df.train.corpus, control = list(weighting = weightTfIdf))
tdm
tdm <- removeSparseTerms(tdm, 0.95)
tdm
inspect(tdm)
frecuencias <- rowSums(as.matrix(tdm))
plot(sort(frecuencias, decreasing = TRUE))
tdm <- TermDocumentMatrix(df.train.corpus, control = list(weighting = weightTfIdf))
tdm
tdm <- removeSparseTerms(tdm, 0.99)
tdm
inspect(tdm)
frecuencias <- rowSums(as.matrix(tdm))
plot(sort(frecuencias, decreasing = TRUE))
tail(sort(frecuencias),n=20)
freq = data.frame(sort(rowSums(as.matrix(tdm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
source("./requirements.R")
# Load of all the libraries
library(data.table)
library(hunspell)
library(qdap)
library(utf8)
library(dplyr)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(wordcloud)
library(tm)
library(caret)
source("./helper.R")
set.seed(957735)
df.train <- fread("./data/train.csv")
df.test <- fread("./data/test.csv")
# Train dataset
df.train$id <- NULL
df.train$keyword <- NULL
df.train$location <- NULL
# Test dataset
df.test$keyword <- NULL
df.test$location <- NULL
df.train$target <- as.factor(df.train$target)
summary(df.train)
df.train$text[!utf8_valid(df.train$text)]
NFC_df <- utf8_normalize(df.train$text)
sum(NFC_df != df.train$text) # It is normalized
NFC_df <- utf8_normalize(df.test$text)
sum(NFC_df != df.test$text) # It is normalized
# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
summary(unlist(strsplit(as.character(df.test$text), split = " ")) %>%
hunspell_check() )
df.train.corpus.original <- Corpus(VectorSource(df.train$text))
df.test.corpus.original <- Corpus(VectorSource(df.test$text))
df.train.corpus <- tm_map(df.train.corpus.original, content_transformer(tolower))
df.test.corpus <- tm_map(df.test.corpus.original, content_transformer(tolower))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removeNumbers))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removeWords), stopwords())
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removePunctuation))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(stripWhitespace))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(stemDocument))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removeNumbers))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removeWords), stopwords())
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removePunctuation))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(stripWhitespace))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(stemDocument))
df.train.corpus.original[['32']][['content']]
df.train.corpus[['32']][['content']]
df.test.corpus.original[['25']][['content']]
df.test.corpus[['25']][['content']]
tdm <- TermDocumentMatrix(df.train.corpus, control = list(weighting = weightTfIdf))
tdm
tdm <- removeSparseTerms(tdm, 0.99)
tdm
inspect(tdm)
frecuencias <- rowSums(as.matrix(tdm))
plot(sort(frecuencias, decreasing = TRUE))
tail(sort(frecuencias),n=20)
freq = data.frame(sort(rowSums(as.matrix(tdm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
upper.bound <- 5000
dfm.train.train <- dfm(corpus(df.train.corpus)[1:upper.bound])
dfm.train.test <- dfm(corpus(df.train.corpus)[upper.bound:length(corpus(df.train.corpus))])
model.svm <- textmodel_svm(dfm.train.train, df.train$target[1:upper.bound])
predictions.svm <- predict(model.svm, newdata=dfm.train.test)
tab_class <- table(df.train$target[upper.bound:length(corpus(df.train.corpus))], predictions.svm)
confusionMatrix(tab_class, mode = "everything")
model.svmlin <- textmodel_svmlin(dfm.train.train, df.train$target[1:upper.bound])
predictions.svmlin <- predict(model.svmlin, newdata=dfm.train.test, force = T)
tab_class <- table(df.train$target[upper.bound:length(corpus(df.train.corpus))], predictions.svmlin)
confusionMatrix(tab_class, mode = "everything")
model.nb <- textmodel_nb(dfm.train.train, df.train$target[1:upper.bound])
predictions.nb <- predict(model.nb, newdata=dfm.train.test, force = T)
tab_class <- table(df.train$target[upper.bound:length(corpus(df.train.corpus))], predictions.nb)
confusionMatrix(tab_class, mode = "everything")
dfm.train <- dfm(corpus(df.train.corpus))
dfm.test <- dfm(corpus(df.test.corpus))
model.svm <- textmodel_svm(dfm.train, df.train$target)
model.svmlin <- textmodel_svmlin(dfm.train, df.train$target)
model.nb <- textmodel_nb(dfm.train, df.train$target)
predictions.svm <- predict(model.svm, newdata=dfm.test)
predictions.svmlin <- predict(model.svmlin, newdata=dfm.test, force = T)
predictions.nb <- predict(model.nb, newdata=dfm.test, force = T)
df.test.svm <- data.frame(
id = df.test$id,
target = predictions.svm
)
df.test.svmlin <- data.frame(
id = df.test$id,
target = predictions.svmlin
)
df.test.nb <- data.frame(
id = df.test$id,
target = predictions.nb
)
write.csv(df.test.svm,
"./output/svm.test.csv",
sep = ",",
col.names = T,
row.names = F)
write.csv(df.test.svmlin,
"./output/svmlin.test.csv",
sep = ",",
col.names = T,
row.names = F)
write.csv(df.test.nb,
"./output/nb.test.csv",
sep = ",",
col.names = T,
row.names = F)
source("./requirements.R")
# Load of all the libraries
library(data.table)
library(hunspell)
library(qdap)
library(utf8)
library(dplyr)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(wordcloud)
library(tm)
library(caret)
source("./helper.R")
set.seed(957735)
df.train <- fread("./data/train.csv")
df.test <- fread("./data/test.csv")
# Train dataset
df.train$id <- NULL
df.train$keyword <- NULL
df.train$location <- NULL
# Test dataset
df.test$keyword <- NULL
df.test$location <- NULL
df.train$target <- as.factor(df.train$target)
summary(df.train)
df.train$text[!utf8_valid(df.train$text)]
NFC_df <- utf8_normalize(df.train$text)
sum(NFC_df != df.train$text) # It is normalized
NFC_df <- utf8_normalize(df.test$text)
sum(NFC_df != df.test$text) # It is normalized
# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
summary(unlist(strsplit(as.character(df.test$text), split = " ")) %>%
hunspell_check() )
df.train.corpus.original <- Corpus(VectorSource(df.train$text))
df.test.corpus.original <- Corpus(VectorSource(df.test$text))
df.train.corpus <- tm_map(df.train.corpus.original, content_transformer(tolower))
df.test.corpus <- tm_map(df.test.corpus.original, content_transformer(tolower))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removeNumbers))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removeWords), stopwords())
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removePunctuation))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(stripWhitespace))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(stemDocument))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removeNumbers))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removeWords), stopwords())
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removePunctuation))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(stripWhitespace))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(stemDocument))
df.train.corpus.original[['32']][['content']]
df.train.corpus[['32']][['content']]
df.test.corpus.original[['25']][['content']]
df.test.corpus[['25']][['content']]
tdm <- TermDocumentMatrix(df.train.corpus, control = list(weighting = weightTfIdf))
tdm
tdm <- removeSparseTerms(tdm, 0.99)
tdm
inspect(tdm)
frecuencias <- rowSums(as.matrix(tdm))
plot(sort(frecuencias, decreasing = TRUE))
tail(sort(frecuencias),n=20)
freq = data.frame(sort(rowSums(as.matrix(tdm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
upper.bound <- 5000
dfm.train.train <- dfm(corpus(df.train.corpus)[1:upper.bound])
dfm.train.test <- dfm(corpus(df.train.corpus)[upper.bound:length(corpus(df.train.corpus))])
model.svm <- textmodel_svm(dfm.train.train, df.train$target[1:upper.bound])
predictions.svm <- predict(model.svm, newdata=dfm.train.test)
tab_class <- table(df.train$target[upper.bound:length(corpus(df.train.corpus))], predictions.svm)
confusionMatrix(tab_class, mode = "everything")
model.svmlin <- textmodel_svmlin(dfm.train.train, df.train$target[1:upper.bound])
predictions.svmlin <- predict(model.svmlin, newdata=dfm.train.test, force = T)
tab_class <- table(df.train$target[upper.bound:length(corpus(df.train.corpus))], predictions.svmlin)
confusionMatrix(tab_class, mode = "everything")
model.nb <- textmodel_nb(dfm.train.train, df.train$target[1:upper.bound])
predictions.nb <- predict(model.nb, newdata=dfm.train.test, force = T)
tab_class <- table(df.train$target[upper.bound:length(corpus(df.train.corpus))], predictions.nb)
confusionMatrix(tab_class, mode = "everything")
dfm.train <- dfm(corpus(df.train.corpus))
dfm.test <- dfm(corpus(df.test.corpus))
model.svm <- textmodel_svm(dfm.train, df.train$target)
model.svmlin <- textmodel_svmlin(dfm.train, df.train$target)
model.nb <- textmodel_nb(dfm.train, df.train$target)
predictions.svm <- predict(model.svm, newdata=dfm.test)
predictions.svmlin <- predict(model.svmlin, newdata=dfm.test, force = T)
predictions.nb <- predict(model.nb, newdata=dfm.test, force = T)
df.test.svm <- data.frame(
id = df.test$id,
target = predictions.svm
)
df.test.svmlin <- data.frame(
id = df.test$id,
target = predictions.svmlin
)
df.test.nb <- data.frame(
id = df.test$id,
target = predictions.nb
)
write.csv(df.test.svm,
"./output/svm.test.csv",
sep = ",",
col.names = T,
row.names = F)
write.csv(df.test.svmlin,
"./output/svmlin.test.csv",
sep = ",",
col.names = T,
row.names = F)
write.csv(df.test.nb,
"./output/nb.test.csv",
sep = ",",
col.names = T,
row.names = F)
corpus(tfm)
corpus(tdm)
corpus(as.character(tdm))
length(df.train.corpus)
length(df.train.corpus)*0.7
upper.bound <- round(length(df.train.corpus)*0.7,0)
dfm.train.train <- dfm(corpus(df.train.corpus)[1:upper.bound])
dfm.train.test <- dfm(corpus(df.train.corpus)[upper.bound:length(corpus(df.train.corpus))])
source("./requirements.R")
# Load of all the libraries
library(data.table)
library(hunspell)
library(qdap)
library(utf8)
library(dplyr)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(wordcloud)
library(tm)
library(caret)
source("./helper.R")
set.seed(957735)
df.train <- fread("./data/train.csv")
df.test <- fread("./data/test.csv")
# Train dataset
df.train$id <- NULL
df.train$keyword <- NULL
df.train$location <- NULL
# Test dataset
df.test$keyword <- NULL
df.test$location <- NULL
df.train$target <- as.factor(df.train$target)
summary(df.train)
df.train$text[!utf8_valid(df.train$text)]
NFC_df <- utf8_normalize(df.train$text)
sum(NFC_df != df.train$text) # It is normalized
NFC_df <- utf8_normalize(df.test$text)
sum(NFC_df != df.test$text) # It is normalized
# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
summary(unlist(strsplit(as.character(df.test$text), split = " ")) %>%
hunspell_check() )
df.train.corpus.original <- Corpus(VectorSource(df.train$text))
df.test.corpus.original <- Corpus(VectorSource(df.test$text))
df.train.corpus <- tm_map(df.train.corpus.original, content_transformer(tolower))
df.test.corpus <- tm_map(df.test.corpus.original, content_transformer(tolower))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removeNumbers))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removeWords), stopwords())
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removePunctuation))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(stripWhitespace))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(stemDocument))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removeNumbers))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removeWords), stopwords())
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removePunctuation))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(stripWhitespace))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(stemDocument))
df.train.corpus.original[['32']][['content']]
df.train.corpus[['32']][['content']]
df.test.corpus.original[['25']][['content']]
df.test.corpus[['25']][['content']]
tdm <- TermDocumentMatrix(df.train.corpus, control = list(weighting = weightTfIdf))
tdm
tdm <- removeSparseTerms(tdm, 0.99)
tdm
inspect(tdm)
frecuencias <- rowSums(as.matrix(tdm))
plot(sort(frecuencias, decreasing = TRUE))
tail(sort(frecuencias),n=20)
freq = data.frame(sort(rowSums(as.matrix(tdm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
upper.bound <- round(length(df.train.corpus)*0.7,0)
dfm.train.train <- dfm(corpus(df.train.corpus)[1:upper.bound])
dfm.train.test <- dfm(corpus(df.train.corpus)[upper.bound:length(corpus(df.train.corpus))])
model.svm <- textmodel_svm(dfm.train.train, df.train$target[1:upper.bound])
predictions.svm <- predict(model.svm, newdata=dfm.train.test)
tab_class <- table(df.train$target[upper.bound:length(corpus(df.train.corpus))], predictions.svm)
confusionMatrix(tab_class, mode = "everything")
model.svmlin <- textmodel_svmlin(dfm.train.train, df.train$target[1:upper.bound])
predictions.svmlin <- predict(model.svmlin, newdata=dfm.train.test, force = T)
tab_class <- table(df.train$target[upper.bound:length(corpus(df.train.corpus))], predictions.svmlin)
confusionMatrix(tab_class, mode = "everything")
model.nb <- textmodel_nb(dfm.train.train, df.train$target[1:upper.bound])
predictions.nb <- predict(model.nb, newdata=dfm.train.test, force = T)
tab_class <- table(df.train$target[upper.bound:length(corpus(df.train.corpus))], predictions.nb)
confusionMatrix(tab_class, mode = "everything")
dfm.train <- dfm(corpus(df.train.corpus))
dfm.test <- dfm(corpus(df.test.corpus))
model.svm <- textmodel_svm(dfm.train, df.train$target)
model.svmlin <- textmodel_svmlin(dfm.train, df.train$target)
model.nb <- textmodel_nb(dfm.train, df.train$target)
predictions.svm <- predict(model.svm, newdata=dfm.test)
predictions.svmlin <- predict(model.svmlin, newdata=dfm.test, force = T)
predictions.nb <- predict(model.nb, newdata=dfm.test, force = T)
df.test.svm <- data.frame(
id = df.test$id,
target = predictions.svm
)
df.test.svmlin <- data.frame(
id = df.test$id,
target = predictions.svmlin
)
df.test.nb <- data.frame(
id = df.test$id,
target = predictions.nb
)
write.csv(df.test.svm,
"./output/svm.test.csv",
sep = ",",
col.names = T,
row.names = F)
write.csv(df.test.svmlin,
"./output/svmlin.test.csv",
sep = ",",
col.names = T,
row.names = F)
write.csv(df.test.nb,
"./output/nb.test.csv",
sep = ",",
col.names = T,
row.names = F)
