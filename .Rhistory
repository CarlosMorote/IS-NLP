library(plotly)
library(htmlwidgets)
library(IRdisplay)
library(hunspell)
library(gridExtra)
library(DT)
voc_size <- tr_val$text %>%
as.character() %>%
paste(., collapse = " ") %>%
strsplit(., split = " ") %>%
unlist() %>%
factor() %>%
unique() %>%
length()
tokenizer <- text_tokenizer(num_words = voc_size)
# Load of all the libraries
set.seed(957735)
library(data.table)
#library(tm)
#library(ggplot2)
#library(ggwordcloud)
library(keras)
#library(tensorflow)
#library(plotly)
#library(htmlwidgets)
#library(IRdisplay)
library(hunspell)
#library(gridExtra)
#library(DT)
library(qdap)
library(utf8)
library(dplyr)
library(spacyr)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(tm)
source("./helper.R")
df.train <- fread("./data/train.csv")
df.train$id <- NULL
df.train$keyword <- NULL
df.train$location <- NULL
df.train = df.train[df.train$target != ""] # Remove those instances that has is class as NA
df.train$target <- as.factor(df.train$target)
df.test <- fread("./data/test.csv")
df.test$id <- NULL
df.test$keyword <- NULL
df.test$location <- NULL
summary(df.train)
df.train$text[!utf8_valid(df.train$text)]
NFC_df <- utf8_normalize(df.train$text)
sum(NFC_df != df.train$text) # It is normalized
# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
# Load of all the libraries
set.seed(957735)
library(data.table)
#library(tm)
#library(ggplot2)
#library(ggwordcloud)
library(keras)
#library(tensorflow)
#library(plotly)
#library(htmlwidgets)
#library(IRdisplay)
library(hunspell)
#library(gridExtra)
#library(DT)
library(qdap)
library(utf8)
library(dplyr)
library(spacyr)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(tm)
source("./helper.R")
df.train <- fread("./data/train.csv")
df.train$id <- NULL
df.train$keyword <- NULL
df.train$location <- NULL
df.train = df.train[df.train$target != ""] # Remove those instances that has is class as NA
df.train$target <- as.factor(df.train$target)
df.test <- fread("./data/test.csv")
df.test$id <- NULL
df.test$keyword <- NULL
df.test$location <- NULL
summary(df.train)
df.train$text[!utf8_valid(df.train$text)]
NFC_df <- utf8_normalize(df.train$text)
sum(NFC_df != df.train$text) # It is normalized
# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
df.corpus.original <- Corpus(VectorSource(df.train$text))
df.corpus <- tm_map(df.corpus.original, content_transformer(tolower))
# 1
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
df.corpus <- tm_map(df.corpus.original, content_transformer(tolower))
# 1
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
df.corpus.original <- Corpus(VectorSource(df.train$text))
df.corpus <- tm_map(df.corpus.original, content_transformer(tolower))
View(df.corpus)
# 1
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
# 32
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
# 32
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
# 40
df.corpus <- tm_map(df.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.corpus <- tm_map(df.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.corpus <- tm_map(df.corpus, content_transformer(removeNumbers))
df.corpus <- tm_map(df.corpus, content_transformer(removeWords), stopwords())
df.corpus <- tm_map(df.corpus, content_transformer(removePunctuation))
df.corpus <- tm_map(df.corpus, content_transformer(stripWhitespace))
df.corpus <- tm_map(df.corpus, content_transformer(stemDocument))
df.corpus.original[['32']][['content']]
df.corpus[['32']][['content']]
tdm <- TermDocumentMatrix(df.corpus, control = list(weighting = weightTfIdf))
tdm
frecuencias <- rowSums(as.matrix(tdm))
plot(sort(frecuencias, decreasing = TRUE))
tail(sort(frecuencias),n=10)
text <- unlist(df.corpus)
text <- as.data.frame(text)
text <- text[1:(nrow(text)-1), ]
text <- as.data.frame(text)
tail(text)
val <- data.frame(df.train$target, text$text)
colnames(val) <- c("target", "text")
head(val)
ttPart <- .7
u <- runif(n = nrow(val), min = 0, max = 1)
tr_val <- val[u <= ttPart,]
te_val <- val[u > ttPart,]
voc_size <- tr_val$text %>%
as.character() %>%
paste(., collapse = " ") %>%
strsplit(., split = " ") %>%
unlist() %>%
factor() %>%
unique() %>%
length()
tokenizer <- text_tokenizer(num_words = voc_size)
n
tokenizer %>%  fit_text_tokenizer(tr_val$text)
tokenizer$word_index %>% head(., n=6)
batch_size = 128
eps = 45
latent_size = 3
clip = 3
leRa = .0025
dec = .00001
text_seqs_val_tr <- texts_to_sequences(tokenizer, tr_val$text)
text_seqs_val_te <- texts_to_sequences(tokenizer, te_val$text)
head(text_seqs_val_tr); head(text_seqs_val_te)
inputShape = 120
x_train <- text_seqs_val_tr %>% pad_sequences(maxlen = inputShape, padding = "post")
x_test <- text_seqs_val_te %>% pad_sequences(maxlen = inputShape, padding = "post")
dim(x_train); dim(x_test)
head(x_train); head(x_test)
text_seqs_val_tr <- texts_to_sequences(tokenizer, tr_val$text)
text_seqs_val_te <- texts_to_sequences(tokenizer, te_val$text)
inputShape = 120
x_train <- text_seqs_val_tr %>% pad_sequences(maxlen = inputShape, padding = "post")
x_test <- text_seqs_val_te %>% pad_sequences(maxlen = inputShape, padding = "post")
dim(x_train); dim(x_test)
y_train <- tr_val$target
batch_size = 128
eps = 45
latent_size = 3
clip = 3
leRa = .0025
dec = .00001
ENCODER_in <- layer_input(shape = c(inputShape))
ENCODER_out = ENCODER_in %>%
layer_embedding(input_dim = voc_size,
output_dim = 30) %>%
layer_dropout(.2) %>%
layer_global_max_pooling_1d() %>%
layer_batch_normalization() %>%
layer_dense(units=120) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=60) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=latent_size) %>%
layer_activation_leaky_relu()
ENCODER = keras_model(ENCODER_in, ENCODER_out)
summary(ENCODER)
DECODER_in = layer_input(shape = latent_size)
DECODER_out = DECODER_in %>%
layer_batch_normalization() %>%
layer_dense(units=60) %>%
layer_activation_leaky_relu() %>%
layer_dense(units=120) %>%
layer_activation_leaky_relu() %>%
layer_dense(units = inputShape, activation = "relu") # Because output must be >= 0
DECODER = keras_model(DECODER_in, DECODER_out)
summary(DECODER)
AEN_in = layer_input(shape = inputShape)
AEN_out = AEN_in %>%
ENCODER() %>%
DECODER()
AEN = keras_model(AEN_in, AEN_out)
summary(AEN)
rmse <- function(y_pred, y_true){
y_pred = k_cast(y_pred, dtype="float32")
y_true = k_cast(y_true, dtype="float32")
rmse = k_sqrt(k_mean(k_square(y_pred - y_true), axis=-1))
return(rmse)
}
AEN %>% keras::compile(
loss = rmse,
optimizer = optimizer_adamax(beta_2 = .95, lr = leRa, decay = dec),
metrics = c('accuracy')
)
AEN %>% keras::compile(
loss = rmse,
optimizer = optimizer_adamax(beta_2 = .95, learning_rate = leRa, decay = dec),
metrics = c('accuracy')
)
stop <- callback_early_stopping(monitor = 'accuracy', patience = 10)
hist <- AEN %>%
fit(
x_train,
x_train,
batch_size = batch_size,
epochs = eps,
callbacks = c(stop)
)
encoded = ENCODER %>%
predict(x_train)
decoded = DECODER %>%
predict(encoded) %>%
as.data.frame()
head(decoded)
View(x_train)
# Load of all the libraries
set.seed(957735)
library(data.table)
library(qdap)
library(utf8)
library(dplyr)
library(spacyr)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(tm)
source("./helper.R")
df.train <- fread("./data/train.csv")
df.train$id <- NULL
df.train$keyword <- NULL
df.train$location <- NULL
df.train = df.train[df.train$target != ""] # Remove those instances that has is class as NA
df.train$target <- as.factor(df.train$target)
df.test <- fread("./data/test.csv")
df.test$id <- NULL
df.test$keyword <- NULL
df.test$location <- NULL
summary(df.train)
df.train$text[!utf8_valid(df.train$text)]
NFC_df <- utf8_normalize(df.train$text)
sum(NFC_df != df.train$text) # It is normalized
# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
library(hunspell)
# Detects spelling errors
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
df.corpus.original <- Corpus(VectorSource(df.train$text))
df.corpus <- tm_map(df.corpus.original, content_transformer(tolower))
# 1
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
# 32
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
# 32
df.corpus <- tm_map(df.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
# 40
df.corpus <- tm_map(df.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.corpus <- tm_map(df.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.corpus <- tm_map(df.corpus, content_transformer(removeNumbers))
df.corpus <- tm_map(df.corpus, content_transformer(removeWords), stopwords())
df.corpus <- tm_map(df.corpus, content_transformer(removePunctuation))
df.corpus <- tm_map(df.corpus, content_transformer(stripWhitespace))
df.corpus <- tm_map(df.corpus, content_transformer(stemDocument))
df.corpus.original[['32']][['content']]
df.corpus[['32']][['content']]
tdm <- TermDocumentMatrix(df.corpus, control = list(weighting = weightTfIdf))
tdm
frecuencias <- rowSums(as.matrix(tdm))
plot(sort(frecuencias, decreasing = TRUE))
tail(sort(frecuencias),n=10)
dfm.train <- dfm(corpus(df.corpus))
# dfm.test <- dfm(df_test$text)
model.svm <- textmodel_svm(dfm.train, df.train$target)
model.nb <- textmodel_nb(dfm.train, df.train$target)
predictions.svm <- predict(model.svm, newdata=dfm.train)
predictions.nb <- predict(model.nb, newdata=dfm.train)
# Load of all the libraries
set.seed(957735)
library(data.table)
library(hunspell)
library(qdap)
library(utf8)
library(dplyr)
library(spacyr)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(tm)
source("./helper.R")
df.train <- fread("./data/train.csv")
df.train$id <- NULL
df.train$keyword <- NULL
df.train$location <- NULL
df.train = df.train[df.train$target != ""] # Remove those instances that has is class as NA
df.train$target <- as.factor(df.train$target)
df.test <- fread("./data/test.csv")
df.test$id <- NULL
df.test$keyword <- NULL
df.test$location <- NULL
summary(df.train)
df.train$text[!utf8_valid(df.train$text)]
NFC_df <- utf8_normalize(df.train$text)
sum(NFC_df != df.train$text) # It is normalized
NFC_df <- utf8_normalize(df.test$text)
sum(NFC_df != df.test$text)
summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
summary(unlist(strsplit(as.character(df.test$text), split = " ")) %>%
hunspell_check() )
df.train.corpus.original <- Corpus(VectorSource(df.train$text))
df.test.corpus.original <- Corpus(VectorSource(df.test$text))
df.train.corpus <- tm_map(df.train.corpus.original, content_transformer(tolower))
df.test.corpus <- tm_map(df.test.corpus.original, content_transformer(tolower))
# 1
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
# 32
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
# 32
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
# 40
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removeNumbers))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removeWords), stopwords())
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removePunctuation))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(stripWhitespace))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(stemDocument))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removeNumbers))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removeWords), stopwords())
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removePunctuation))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(stripWhitespace))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(stemDocument))
df.train.corpus.original[['32']][['content']]
df.train.corpus[['32']][['content']]
df.test.corpus.original[['32']][['content']]
df.test.corpus[['32']][['content']]
tdm <- TermDocumentMatrix(df.train.corpus, control = list(weighting = weightTfIdf))
tdm
frecuencias <- rowSums(as.matrix(tdm))
plot(sort(frecuencias, decreasing = TRUE))
tail(sort(frecuencias),n=20)
dfm.train <- dfm(corpus(df.train.corpus))
dfm.test <- dfm(corpus(df.train.corpus))
model.svm <- textmodel_svm(dfm.train, df.train$target)
model.nb <- textmodel_nb(dfm.train, df.train$target)
predictions.svm <- predict(model.svm, newdata=dfm.train)
predictions.nb <- predict(model.nb, newdata=dfm.train)
dfm.train <- dfm(corpus(df.train.corpus))
dfm.test <- dfm(corpus(df.train.corpus))
model.svm <- textmodel_svm(dfm.train, df.train$target)
model.nb <- textmodel_nb(dfm.train, df.train$target)
predictions.svm <- predict(model.svm, newdata=dfm.test)
predictions.nb <- predict(model.nb, newdata=dfm.test)
df.test.svm <- data.frame(
id = df.test$id,
target = predictions.svm
)
View(predictions.svm)
head(predictions.svm)
head(predictions.svm[,1:])
head(predictions.svm)
as.list(predictions.svm)
table(predictions.svm)
levels(predictions.svm)
data.frame(predictions.svm)
data.frame(predictions.svm)[2]
data.frame(predictions.svm)[[2]]
s.aux <- data.frame(predictions.svm)
View(s.aux)
df.test.svm <- data.frame(
id = df.test$id,
target = data.frame(predictions.svm)$predictions.svm
)
dfm.train <- dfm(corpus(df.train.corpus))
dfm.test <- dfm(corpus(df.test.corpus))
model.svm <- textmodel_svm(dfm.train, df.train$target)
model.nb <- textmodel_nb(dfm.train, df.train$target)
predictions.svm <- predict(model.svm, newdata=dfm.test)
predictions.nb <- predict(model.nb, newdata=dfm.test)
df.test.svm <- data.frame(
id = df.test$id,
target = predictions.svm
)
s.aux <- data.frame(predictions.svm)
df.test.svm <- data.frame(
id = df.test$id,
target = data.frame(predictions.svm)$predictions.svm
)
df.test.svm <- data.frame(
id = df.test$id,
target = predictions.svm
)
View(model.nb)
df.test.svm <- data.frame(
"id" = df.test$id,
"target" = predictions.svm
)
dim(df.test$id)
lenght(df.test$id)
length(df.test$id)
df.test$id
df.test$keyword <- NULL
df.test <- fread("./data/test.csv")
df.test$keyword <- NULL
df.test$location <- NULL
df.test.svm <- data.frame(
id = df.test$id,
target = predictions.svm
)
df.test.nb <- data.frame(
id = df.test$id,
target = predictions.nb
)
write.csv2(df.test.svm,
"./svm.test.csv",
col.names = F)
write.csv2(df.test.svm,
"./output/svm.test.csv",
col.names = F)
write.csv2(df.test.svm,
"./output/svm.test.csv",
col.names = F,
row.names = F)
write.csv2(df.test.svm,
"./output/svm.test.csv",
col.names = F,
row.names = F,
sep = ",")
write.csv2(df.test.svm,
"./output/svm.test.csv",
col.names = T,
row.names = F,
sep = ",")
write.csv2(df.test.svm,
"./output/svm.test.csv",
sep = ",",
col.names = T,
row.names = F)
write.csv(df.test.svm,
"./output/svm.test.csv",
sep = ",",
col.names = T,
row.names = F)
predictions.nb <- predict(model.nb, newdata=dfm.test)
model.svmlin <- textmodel_svmlin(dfm.train, df.train$target)
predictions.svmlin <- predict(model.svmlin, newdata=dfm.test)
dfm.test <- dfm_select(dfm(corpus(df.test.corpus)), dfm.train)
dfm.test <- dfm(corpus(df.test.corpus))
predictions.svmlin <- predict(model.svmlin, newdata=dfm.test)
predictions.svmlin <- predict(model.svmlin, newdata=dfm.test, force = T)
predictions.nb <- predict(model.nb, newdata=dfm.test, force = T)
df.test.svm <- data.frame(
id = df.test$id,
target = predictions.svm
)
df.test.svmlin <- data.frame(
id = df.test$id,
target = predictions.svmlin
)
df.test.nb <- data.frame(
id = df.test$id,
target = predictions.nb
)
write.csv(df.test.svm,
"./output/svm.test.csv",
sep = ",",
col.names = T,
row.names = F)
write.csv(df.test.svmlin,
"./output/svmlin.test.csv",
sep = ",",
col.names = T,
row.names = F)
write.csv(df.test.nb,
"./output/nb.test.csv",
sep = ",",
col.names = T,
row.names = F)
