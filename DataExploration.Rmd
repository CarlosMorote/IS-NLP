---
title: "R Notebook"
author: "Carlos Morote Garc√≠a"
output: html_notebook
---

Data Exploration to understand the data

```{r}
# Load of all the libraries

set.seed(957735)

library(data.table)
library(hunspell)
library(qdap)

library(utf8)
library(dplyr)
library(spacyr)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)

library(tm)
library(caret)

source("./helper.R")
```

Once the libraries are loaded we load the data

```{r}
df.train <- fread("./data/train.csv")
df.train$id <- NULL
df.train$keyword <- NULL
df.train$location <- NULL
df.train = df.train[df.train$target != ""] # Remove those instances that has is class as NA
df.train$target <- as.factor(df.train$target)
```

```{r}
df.test <- fread("./data/test.csv")
df.test$keyword <- NULL
df.test$location <- NULL
```


```{r}
summary(df.train)
```

Check it the codification is properlly done in _utf-8_

```{r}
df.train$text[!utf8_valid(df.train$text)]
```

Normalize the text

```{r}
NFC_df <- utf8_normalize(df.train$text)
sum(NFC_df != df.train$text) # It is normalized
```

```{r}
NFC_df <- utf8_normalize(df.test$text)
sum(NFC_df != df.test$text)
```


Study how many spelling errors there are across the tweets. There are 24308 spelling errors. This may include: urls, emojis, hashtags (#) and mentions to other accounts (@).

```{r}
# Detects spelling errors

summary(unlist(strsplit(as.character(df.train$text), split = " ")) %>%
hunspell_check() )
```

```{r}
summary(unlist(strsplit(as.character(df.test$text), split = " ")) %>%
hunspell_check() )
```


```{r}
df.train.corpus.original <- Corpus(VectorSource(df.train$text))
df.test.corpus.original <- Corpus(VectorSource(df.test$text))
```


All to lower casses

```{r}
df.train.corpus <- tm_map(df.train.corpus.original, content_transformer(tolower))
df.test.corpus <- tm_map(df.test.corpus.original, content_transformer(tolower))
```


Remove the character `#`

```{r}
# 1
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("[#]{1,}([A-Z][^A-Z]*)+", "\\1", text)}))
```

Remove the mentions (`@`)

```{r}
# 32
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("@\\S+ ", "", text)}))
```


Remove urls

```{r}
# 32
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){gsub("\\S*http+\\S*", "", text)}))
```

Remove emojis

```{r}
# 40
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){mgsub(text, pattern = emojis, replacement = "")}))
```

Undo the contractions

```{r}
df.train.corpus <- tm_map(df.train.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(function(text){replace_contraction(text, contraction = contra, sent.cap = FALSE)}))
```

Basic final transormations

```{r}
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removeNumbers))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removeWords), stopwords())
df.train.corpus <- tm_map(df.train.corpus, content_transformer(removePunctuation))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(stripWhitespace))
df.train.corpus <- tm_map(df.train.corpus, content_transformer(stemDocument))

df.test.corpus <- tm_map(df.test.corpus, content_transformer(removeNumbers))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removeWords), stopwords())
df.test.corpus <- tm_map(df.test.corpus, content_transformer(removePunctuation))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(stripWhitespace))
df.test.corpus <- tm_map(df.test.corpus, content_transformer(stemDocument))
```

Contrast the original corpus to the processed one

```{r}
df.train.corpus.original[['32']][['content']]
df.train.corpus[['32']][['content']]
```

```{r}
df.test.corpus.original[['32']][['content']]
df.test.corpus[['32']][['content']]
```

TDM

```{r}
tdm <- TermDocumentMatrix(df.train.corpus, control = list(weighting = weightTfIdf))
tdm
```

Most common words

```{r}
frecuencias <- rowSums(as.matrix(tdm))
plot(sort(frecuencias, decreasing = TRUE))
```

```{r}
tail(sort(frecuencias),n=20)
```

---


```{r}
upper.bound <- 5000
dfm.train.train <- dfm(corpus(df.train.corpus)[1:upper.bound])
dfm.train.test <- dfm(corpus(df.train.corpus)[upper.bound:length(corpus(df.train.corpus))])
```

```{r}
model.svm <- textmodel_svm(dfm.train.train, df.train$target[1:upper.bound])

predictions.svm <- predict(model.svm, newdata=dfm.train.test)

tab_class <- table(df.train$target[upper.bound:length(corpus(df.train.corpus))], predictions.svm)
confusionMatrix(tab_class, mode = "everything")
```

```{r}
model.svmlin <- textmodel_svmlin(dfm.train.train, df.train$target[1:upper.bound])

predictions.svmlin <- predict(model.svmlin, newdata=dfm.train.test, force = T)

tab_class <- table(df.train$target[upper.bound:length(corpus(df.train.corpus))], predictions.svmlin)
confusionMatrix(tab_class, mode = "everything")
```

```{r}
model.nb <- textmodel_nb(dfm.train.train, df.train$target[1:upper.bound])

predictions.nb <- predict(model.nb, newdata=dfm.train.test, force = T)

tab_class <- table(df.train$target[upper.bound:length(corpus(df.train.corpus))], predictions.nb)
confusionMatrix(tab_class, mode = "everything")
```

/TODO: ustilizar todo el df para entrenar y generar test final

```{r}
df.test.svm <- data.frame(
  id = df.test$id,
  target = predictions.svm
)

df.test.svmlin <- data.frame(
  id = df.test$id,
  target = predictions.svmlin
)

df.test.nb <- data.frame(
  id = df.test$id,
  target = predictions.nb
)
```

```{r}
write.csv(df.test.svm,
           "./output/svm.test.csv",
           sep = ",",
           col.names = T,
           row.names = F)

write.csv(df.test.svmlin,
           "./output/svmlin.test.csv",
           sep = ",",
           col.names = T,
           row.names = F)

write.csv(df.test.nb,
           "./output/nb.test.csv",
           sep = ",",
           col.names = T,
           row.names = F)
```


